#!/usr/bin/env python3
"""
DOGE/USDTäº¤æ˜“å¯¹è®­ç»ƒè¿›åº¦å¯è§†åŒ–è„šæœ¬
å±•ç¤ºpredictorçš„æŸå¤±å˜åŒ–
"""

import matplotlib.pyplot as plt
import numpy as np
import os

def plot_training_progress():
    """ç»˜åˆ¶Predictorè®­ç»ƒè¿›åº¦å›¾è¡¨"""
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # Predictorè®­ç»ƒæ•°æ® (7è½®)
    predictor_epochs = [1, 2, 3, 4, 5, 6, 7]
    predictor_train_loss = [1.946293, 1.751143, 1.706408, 1.680686, 1.663046, 1.649754, 1.639570]
    predictor_val_loss = [2.021497, 1.986604, 1.974955, 1.971007, 1.971090, 1.972143, 1.974842]
    
    # åˆ›å»ºå•ä¸ªå›¾è¡¨
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # ç»˜åˆ¶è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±æ›²çº¿
    train_line = ax.plot(predictor_epochs, predictor_train_loss, 'o-', linewidth=3, markersize=10, 
                        color='#2E86AB', label='è®­ç»ƒæŸå¤±', markerfacecolor='white', markeredgewidth=2)
    val_line = ax.plot(predictor_epochs, predictor_val_loss, 's-', linewidth=3, markersize=10, 
                      color='#C73E1D', label='éªŒè¯æŸå¤±', markerfacecolor='white', markeredgewidth=2)
    
    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title('DOGE/USDTäº¤æ˜“å¯¹ Predictorè®­ç»ƒæŸå¤±å˜åŒ–', fontsize=18, fontweight='bold', pad=20)
    ax.set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)', fontsize=14)
    ax.set_ylabel('æŸå¤±å€¼ (Loss)', fontsize=14)
    
    # æ·»åŠ ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--')
    
    # æ‰¾å‡ºæœ€ä½³è½®æ¬¡
    best_epoch = predictor_val_loss.index(min(predictor_val_loss)) + 1
    best_val_loss = min(predictor_val_loss)
    
    # æ ‡æ³¨æ¯ä¸ªæ•°æ®ç‚¹çš„æ•°å€¼
    for i, (epoch, train_loss, val_loss) in enumerate(zip(predictor_epochs, predictor_train_loss, predictor_val_loss)):
        # è®­ç»ƒæŸå¤±æ ‡æ³¨
        ax.annotate(f'{train_loss:.3f}', 
                   xy=(epoch, train_loss), 
                   xytext=(0, -20), textcoords='offset points',
                   ha='center', va='top', fontsize=10, color='#2E86AB', fontweight='bold')
        
        # éªŒè¯æŸå¤±æ ‡æ³¨
        ax.annotate(f'{val_loss:.3f}', 
                   xy=(epoch, val_loss), 
                   xytext=(0, 15), textcoords='offset points',
                   ha='center', va='bottom', fontsize=10, color='#C73E1D', fontweight='bold')
    
    # æ ‡æ³¨æœ€ä½³ç‚¹
    ax.annotate(f'â˜… æœ€ä½³æ¨¡å‹\nè½®æ¬¡ {best_epoch}', 
                xy=(best_epoch, best_val_loss), 
                xytext=(best_epoch+1.2, best_val_loss-0.03),
                arrowprops=dict(arrowstyle='->', color='red', lw=2),
                fontsize=12, color='red', fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='yellow', alpha=0.8))
    
    # åªæ ‡æ³¨è¿‡æ‹ŸåˆåŒºåŸŸ (5-7è½®) - è®­ç»ƒæŸå¤±ç»§ç»­ä¸‹é™ï¼ŒéªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡
    ax.axvspan(4.5, 7.5, alpha=0.2, color='red', label='è¿‡æ‹ŸåˆåŒºåŸŸ')
    ax.text(6, 1.68, 'è¿‡æ‹ŸåˆåŒºåŸŸ', fontsize=14, fontweight='bold', color='red', 
            ha='center', va='center', bbox=dict(boxstyle="round,pad=0.3", facecolor='yellow', alpha=0.8))
    
    # æ·»åŠ æ³›åŒ–å·®è·çº¿ (éªŒè¯æŸå¤±ä¸è®­ç»ƒæŸå¤±çš„å·®è·)
    # åœ¨ç¬¬4è½®åæ·»åŠ è­¦æˆ’çº¿ï¼Œæ˜¾ç¤ºå·®è·å¼€å§‹å¢å¤§
    for i, epoch in enumerate(predictor_epochs[4:], 5):
        diff = predictor_val_loss[i-1] - predictor_train_loss[i-1]
        if diff > 0.32:  # å½“å·®è·è¶…è¿‡0.32æ—¶ï¼Œç”¨è™šçº¿è¿æ¥
            ax.plot([epoch, epoch], [predictor_train_loss[i-1], predictor_val_loss[i-1]], 
                   'r--', alpha=0.6, linewidth=2)
    
    # æ·»åŠ è¶‹åŠ¿ç®­å¤´
    # è®­ç»ƒæŸå¤±è¶‹åŠ¿ç®­å¤´
    ax.annotate('', xy=(7, predictor_train_loss[-1]), xytext=(6, predictor_train_loss[-2]),
                arrowprops=dict(arrowstyle='->', color='#2E86AB', lw=3))
    ax.text(6.5, predictor_train_loss[-1]-0.01, 'ç»§ç»­ä¸‹é™', fontsize=10, color='#2E86AB', 
            ha='center', fontweight='bold')
    
    # éªŒè¯æŸå¤±è¶‹åŠ¿ç®­å¤´
    ax.annotate('', xy=(7, predictor_val_loss[-1]), xytext=(6, predictor_val_loss[-2]),
                arrowprops=dict(arrowstyle='->', color='#C73E1D', lw=3))
    ax.text(6.5, predictor_val_loss[-1]+0.01, 'å¼€å§‹ä¸Šå‡', fontsize=10, color='#C73E1D', 
            ha='center', fontweight='bold')
    
    # è®¾ç½®å›¾ä¾‹ä½ç½®åˆ°å·¦ä¸‹è§’ï¼Œé¿å…ä¸å›¾ç‰‡é‡åˆ
    ax.legend(loc='lower left', fontsize=12, framealpha=0.9, 
              bbox_to_anchor=(0.02, 0.02))
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´ï¼Œç»™æ•°å€¼æ ‡æ³¨ç•™å‡ºç©ºé—´
    ax.set_xlim(0.5, 7.5)
    ax.set_ylim(1.58, 2.07)
    
    # ç¾åŒ–åæ ‡è½´
    ax.tick_params(axis='both', which='major', labelsize=12)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡
    save_path = 'doge_outputs/predictor_training_progress.png'
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"ğŸ“Š Predictorè®­ç»ƒè¿›åº¦å›¾è¡¨å·²ä¿å­˜: {save_path}")
    
    plt.show()

def print_training_summary():
    """æ‰“å°è®­ç»ƒæ€»ç»“"""
    print("=" * 80)
    print("ğŸ¯ DOGE/USDTäº¤æ˜“å¯¹æ¨¡å‹è®­ç»ƒæ€»ç»“")
    print("=" * 80)
    
    print("\nğŸ“ˆ DOGE/USDT Predictorè®­ç»ƒ (7è½®):")
    print("   è½®æ¬¡ 1: è®­ç»ƒæŸå¤±=1.946293, éªŒè¯æŸå¤±=2.021497")
    print("   è½®æ¬¡ 2: è®­ç»ƒæŸå¤±=1.751143, éªŒè¯æŸå¤±=1.986604")
    print("   è½®æ¬¡ 3: è®­ç»ƒæŸå¤±=1.706408, éªŒè¯æŸå¤±=1.974955")
    print("   è½®æ¬¡ 4: è®­ç»ƒæŸå¤±=1.680686, éªŒè¯æŸå¤±=1.971007 â­ æœ€ä½³")
    print("   è½®æ¬¡ 5: è®­ç»ƒæŸå¤±=1.663046, éªŒè¯æŸå¤±=1.971090")
    print("   è½®æ¬¡ 6: è®­ç»ƒæŸå¤±=1.649754, éªŒè¯æŸå¤±=1.972143")
    print("   è½®æ¬¡ 7: è®­ç»ƒæŸå¤±=1.639570, éªŒè¯æŸå¤±=1.974842")
    
    print("\nğŸ¯ å…³é”®è§‚å¯Ÿ:")
    print("   â€¢ ç¬¬4è½®è¾¾åˆ°æœ€ä½³éªŒè¯æ€§èƒ½ (éªŒè¯æŸå¤±: 1.971007)")
    print("   â€¢ ä»ç¬¬5è½®å¼€å§‹å‡ºç°è¿‡æ‹Ÿåˆç°è±¡")
    print("   â€¢ è®­ç»ƒæŸå¤±æŒç»­ä¸‹é™ï¼ŒéªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡")
    print("   â€¢ æœ€ä½³æ¨¡å‹æ€§èƒ½ç¨³å®šï¼Œé€‚åˆéƒ¨ç½²ä½¿ç”¨")
    
    print("\nğŸ’¾ å·²ä¿å­˜æ¨¡å‹:")
    print("   â€¢ best_predictor_epoch_4.pt âœ… (æœ€ä½³DOGE/USDTé¢„æµ‹æ¨¡å‹)")
    
    print("=" * 80)

def analyze_overfitting():
    """åˆ†æè¿‡æ‹Ÿåˆæƒ…å†µ"""
    print("\nğŸ” è¿‡æ‹Ÿåˆåˆ†æ:")
    
    predictor_train_loss = [1.946293, 1.751143, 1.706408, 1.680686, 1.663046, 1.649754, 1.639570]
    predictor_val_loss = [2.021497, 1.986604, 1.974955, 1.971007, 1.971090, 1.972143, 1.974842]
    
    print("   è½®æ¬¡  |  è®­ç»ƒæŸå¤±  |  éªŒè¯æŸå¤±  |  å·®å¼‚   |  çŠ¶æ€")
    print("   -----|-----------|-----------|---------|--------")
    
    for i, (train, val) in enumerate(zip(predictor_train_loss, predictor_val_loss), 1):
        diff = val - train
        if diff < 0.30:
            status = "âœ… è‰¯å¥½"
        elif diff < 0.35:
            status = "âš ï¸ è­¦æˆ’"
        else:
            status = "ğŸš¨ è¿‡æ‹Ÿåˆ"
            
        print(f"     {i}   |  {train:.6f} |  {val:.6f} |  {diff:.3f}  |  {status}")
    
    print("\n   è¿‡æ‹Ÿåˆä¸´ç•Œç‚¹: ç¬¬4è½®ä¹‹å")
    print("   å»ºè®®: ä½¿ç”¨ç¬¬4è½®çš„æ¨¡å‹è¿›è¡Œéƒ¨ç½²")

if __name__ == "__main__":
    # æ‰“å°è®­ç»ƒæ€»ç»“
    print_training_summary()
    
    # åˆ†æè¿‡æ‹Ÿåˆ
    analyze_overfitting()
    
    # ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨
    plot_training_progress()
    
    print("\nğŸ‰ è®­ç»ƒè¿›åº¦åˆ†æå®Œæˆï¼")
